{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0989a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "#from langchain_community.llms import Ollama\n",
    "#from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "#from langchain_ollama import OllamaEmbeddings\n",
    "import os\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Literal\n",
    "#from langgraph.graph import END\n",
    "from langgraph.graph import START, END\n",
    "from langgraph.graph import MessagesState, StateGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b17b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model cell\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\"\n",
    "    )\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:4b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e7c14a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'persist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m persist_directory = \u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpersist\u001b[49m()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Chroma DB에 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m개의 문서를 임베딩하여 저장 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Chroma' object has no attribute 'persist'"
     ]
    }
   ],
   "source": [
    "# 1. 문서 로드\n",
    "loader = TextLoader(\"./docs/RFP_requirements.md\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. 문서 나누기\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)\n",
    "#vectorstore.persist()\n",
    "\n",
    "print(f\"✅ Chroma DB에 {len(splits)}개의 문서를 임베딩하여 저장 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332a218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe41c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists('./fastmcp-server/requirments_collection') and len(os.listdir('./fastmcp-server/requirments_collection')) > 0:\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name = 'requirments_collection',\n",
    "        persist_directory = './requirments_collection'\n",
    "    )\n",
    "else:\n",
    "    print(\"⚙️ Chroma DB not found. Creating new DB...\")\n",
    "\n",
    "    # ① 문서 로드\n",
    "    loader = TextLoader(\"docs.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # ② 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # ③ Chroma DB 생성 및 persist\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        split_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory='./requirments_collection'\n",
    "    )\n",
    "    print(\"✅ Chroma DB created and persisted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ba886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "#from utils.config import get_llm\n",
    "from workflow.state import AgentState\n",
    "from langchain import hub\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "AOAI_DEPLOY_EMBED_3_LARGE=os.getenv(\"AOAI_DEPLOY_EMBED_3_LARGE\")\n",
    "AOAI_API_KEY=os.getenv(\"AOAI_API_KEY\")\n",
    "AOAI_ENDPOINT=os.getenv(\"AOAI_ENDPOINT\")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=AOAI_DEPLOY_EMBED_3_LARGE,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    api_key= AOAI_API_KEY,\n",
    "    azure_endpoint=AOAI_ENDPOINT\n",
    "    )\n",
    "\n",
    "if os.path.exists('./income_tax_collection') and len(os.listdir('./income_tax_collection')) > 0:\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name = 'income_tax_collection',\n",
    "        persist_directory = './income_tax_collection'\n",
    "    )\n",
    "else:\n",
    "    print(\"⚙️ Chroma DB not found. Creating new DB...\")\n",
    "\n",
    "    # ① 문서 로드\n",
    "    loader = TextLoader(\"example.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # ② 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # ③ Chroma DB 생성 및 persist\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        split_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory='./income_tax_collection'\n",
    "    )\n",
    "    print(\"✅ Chroma DB created and persisted.\")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "# RAG 프롬프트를 가져옵니다.\n",
    "generate_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain Azure OpenAI 설정\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"AOAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AOAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AOAI_DEPLOY_GPT4O\"),\n",
    "    api_version=os.getenv(\"AOAI_API_VERSION\"),\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문에 기반하여 벡터 스토어에서 관련 문서를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 검색된 문서가 추가된 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state.query  # state에서 사용자의 질문을 추출합니다.\n",
    "    docs = retriever.invoke(query)  # 질문과 관련된 문서를 검색합니다.\n",
    "    #return {'context': docs}  # 검색된 문서를 포함한 state를 반환합니다.\n",
    "\n",
    "    return AgentState(\n",
    "            query=state.query,\n",
    "            context=docs,\n",
    "            rewrite_count=state.rewrite_count\n",
    "        )\n",
    "\n",
    "def generate(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state를 기반으로 RAG 체인을 사용하여 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문과 문맥을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 생성된 응답을 포함하는 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    context = state.context  # state에서 문맥을 추출합니다.\n",
    "    query = state.query      # state에서 사용자의 질문을 추출합니다.\n",
    "    \n",
    "    # RAG 체인을 구성합니다.\n",
    "    rag_chain = generate_prompt | llm\n",
    "    \n",
    "    # 질문과 문맥을 사용하여 응답을 생성합니다.\n",
    "    response = rag_chain.invoke({'question': query, 'context': context})\n",
    "    \n",
    "    #return {'answer': response}  # 생성된 응답을 포함하는 state를 반환합니다.\n",
    "\n",
    "    return AgentState(\n",
    "        query=state.query,\n",
    "        context=state.context,\n",
    "        answer=response.content,\n",
    "        rewrite_count=state.rewrite_count\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# 문서 관련성 판단을 위한 프롬프트를 가져옵니다.\n",
    "doc_relevance_prompt = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def check_doc_relevance(state: AgentState) -> Literal['generate', 'rewrite']:\n",
    "    \"\"\"\n",
    "    주어진 state를 기반으로 문서의 관련성을 판단합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문과 문맥을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        Literal['generate', 'rewrite']: 문서가 관련성이 높으면 'generate', 그렇지 않으면 'rewrite'를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state.query  # state에서 사용자의 질문을 추출합니다.\n",
    "    context = state.context  # state에서 문맥을 추출합니다.\n",
    "\n",
    "    # 문서 관련성 판단 체인을 구성합니다.\n",
    "    doc_relevance_chain = doc_relevance_prompt | llm\n",
    "    \n",
    "    # 질문과 문맥을 사용하여 문서의 관련성을 판단합니다.\n",
    "    response = doc_relevance_chain.invoke({'question': query, 'documents': context})\n",
    "\n",
    "    if state.rewrite_count >= 3:\n",
    "        return \"generate\"\n",
    "        \n",
    "    # 관련성이 높으면 'generate'를 반환하고, 그렇지 않으면 'rewrite'를 반환합니다.\n",
    "    if response['Score'] == 1:\n",
    "        return 'generate'\n",
    "    else:\n",
    "        state.rewrite_count += 1\n",
    "        return 'rewrite'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 사전 정의: 특정 표현을 다른 표현으로 변환하기 위한 사전입니다.\n",
    "dictionary = ['사람과 관련된 표현 -> 거주자']\n",
    "\n",
    "# 프롬프트 템플릿을 생성합니다. 사용자의 질문을 사전을 참고하여 변경합니다.\n",
    "rewrite_prompt = PromptTemplate.from_template(f\"\"\"\n",
    "사용자의 질문을 보고, 우리의 사전을 참고해서 사용자의 질문을 변경해주세요 \n",
    "사전: {dictionary}                                           \n",
    "질문: {{query}}\n",
    "\"\"\")\n",
    "\n",
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문을 사전을 참고하여 변경합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 변경된 질문을 포함하는 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state.query  # state에서 사용자의 질문을 추출합니다.\n",
    "    \n",
    "    # 리라이트 체인을 구성합니다. 프롬프트, LLM, 출력 파서를 연결합니다.\n",
    "    rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 질문을 변경합니다.\n",
    "    response = rewrite_chain.invoke({'query': query})\n",
    "    \n",
    "    #return {'query': response}  # 변경된 질문을 포함하는 state를 반환합니다.\n",
    "\n",
    "    return AgentState(\n",
    "        query=response,\n",
    "        context=state.context,\n",
    "        answer=state.answer,\n",
    "        rewrite_count=state.rewrite_count\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    DuckDuckGo를 이용한 간단한 웹 검색.\n",
    "    \"\"\"\n",
    "    url = f\"https://html.duckduckgo.com/html/?q={query}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        results = soup.find_all(\"a\", class_=\"result__a\")\n",
    "        snippets = [a.text for a in results[:3]]\n",
    "        return \"\\n\".join(snippets) if snippets else \"검색 결과가 없습니다.\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"검색 중 오류가 발생했습니다.\"\n",
    "\n",
    "# 추가 한거\n",
    "web_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[web_search],\n",
    "    prompt=\"당신은 최신 정보를 찾기 위해 웹 검색을 수행하는 웹봇입니다. 사용자 질문을 검색해서 답변하세요.\"\n",
    ")\n",
    "\n",
    "def web_node(state: AgentState) -> AgentState:\n",
    "    # DuckDuckGo tool 호출\n",
    "    search_result = web_search(state.query)\n",
    "\n",
    "    web_prompt = f\"\"\"\n",
    "    당신은 최신 정보를 찾기 위해 웹 검색을 수행하는 웹봇입니다.\n",
    "    아래 검색 결과를 참고하여 사용자 질문에 답변하세요.\n",
    "\n",
    "    검색결과:\n",
    "    {search_result}\n",
    "\n",
    "    질문: {state.query}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=web_prompt)])\n",
    "    text = getattr(response, \"content\", str(response))\n",
    "\n",
    "    return AgentState(\n",
    "        query=state.query,\n",
    "        context=state.context,\n",
    "        answer=text,\n",
    "        rewrite_count=state.rewrite_count\n",
    "    )\n",
    "\n",
    "\n",
    "#def web_node(state: AgentState) -> AgentState:\n",
    "#    result = web_agent.invoke(state)\n",
    "#    return Command(\n",
    "#        update={\n",
    "#            \"messages\": [\n",
    "#                HumanMessage(\n",
    "#                    content=result[\"messages\"][-1].content,\n",
    "#                    name=\"web_search\"\n",
    "#                )\n",
    "#            ]\n",
    "#        },\n",
    "#        goto=\"supervisor\",\n",
    "#    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
